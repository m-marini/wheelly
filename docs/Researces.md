# Research Areas

- [x] Remove episode management (only continuous task)
- [x] Training on trajectory sampling with $T=2048$ number of steps, $K$ epochs, $(N  = 32) \le T$ batch size
- [x] Proximal Policy Optimization
- [ ] Adams implementation
- [ ] Attention layer from LLM
- [ ] Full actions output encoding (24 x 9 x 7 = 1512 actions)
- [ ] Reduced KP output actions (# action, tot prob)
- [ ] Temporal Abstraction via Options, Sutton and Burto (2018), Reinforcement Learning, ยง17.2
